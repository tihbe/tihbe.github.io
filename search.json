[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Ismael Balafrej",
    "section": "",
    "text": "Boosting RSNNs for Long-Term Memory\n\n1 min\n\n\npublication\n\nsnn\n\n\n\nBecause even spiking neurons deserve a memory boost!\n\n\n\nSep 5, 2025\n\n\n\n\n\n\n\n\n\n\nThe Growing Challenge of Deepfakes and the Future of Detection\n\n2 min\n\n\npublication\n\n\n\nDeepfakes are getting smarter; so should detection!\n\n\n\nFeb 1, 2025\n\n\n\n\n\n\n\n\n\n\nAre electric cars a worse investment ?\n\n5 min\n\n\nData Science\n\n\n\nThis post examines the value depreciation of electric vehicle (EV) in the Montreal area, with an analysis of the impact of the Quebec government‚Äôs decision to phase out the‚Ä¶\n\n\n\nApr 11, 2024\n\n\n\n\n\n\n\n\n\n\nUnexpected coin flip experiment\n\n3 min\n\n\nsimulation\n\nstatistics\n\n\n\nIn this post, I explore a seemingly straightforward coin flip game between two players. Interestingly, the intuitive approach fails when validated with a Monte Carlo‚Ä¶\n\n\n\nMar 20, 2024\n\n\n\n\n\n\n\n\n\n\nWhat if you could grow an AI chip ? üß†\n\n1 min\n\n\npublication\n\n\n\nNew publication alert !\n\n\n\nDec 8, 2023\n\n\n\n\n\n\n\n\n\n\nPhD thesis defended !\n\n4 min\n\n\nPhD\n\nECG\n\n\n\nLike a proper nerd, I wore an ECG during my thesis defense. Here is the data !\n\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\n\n\nSelection of a timestep for SNN simulation\n\n8 min\n\n\nSNN\n\nLIF\n\n\n\nWhat is the proper timestep to select when simulating a spiking neural network ? The answer is, of course, it depends. Although, I think the usual assumption is incorrect‚Ä¶\n\n\n\nMar 19, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Here is a summary of my academic publications. You may also find new articles in my Google Scholar  or Semantic Scholar  profiles.\n\nPublications\n\nBalafrej, I., Bahadi, S.. Rouat, J., and Alibart, F. Enhancing Temporal Learning in Recurrent Spiking Networks for Neuromorphic Applications. Neuromorph. Comput. Eng. (2025) https://doi.org/10.1088/2634-4386/add293\n[In review] Garg, N., Balafrej, I., Palhares, J., Begon-Lours, L., Florini, D., Falcone, D., Stecconi, T., Bragaglia, V., Offrein, B., Portal, J.-M., Querlioz, D., Beilliard, Y., Drouin, D., Alibart, F. Unsupervised Local Learning Based on Voltage-Dependent Synaptic Plasticity for Resistive and Ferroelectric Synapses. (2025) https://dx.doi.org/10.21203/rs.3.rs-5295706/v1\nBalafrej, I., Dahmane, M. Enhancing Practicality and Efficiency of Deepfake Detection. Scientific Reports. (2024). https://doi.org/10.1038/s41598-024-82223-y.\nBalafrej, I., Janzakova, K., Kumar, A., Garg, N., Scholaert, C., Rouat, J., Drouin, D., Coffinier, Y., Pecqueur, S., and Alibart, F. Structural Plasticity for Neuromorphic Networks with Electropolymerized Dendritic PEDOT Connections. Nature Communications. (2023) https://doi.org/10.1038/s41467-023-43887-8\n[In review] Balafrej, I., Alibart, F. & Rouat, J. Expanding memory in recurrent spiking networks (2023) https://doi.org/10.48550/arXiv.2310.19067\nGoupy, G., Juneau-Fecteau, A., Garg, N., Balafrej, I., Alibart, F., Frechette, L., Drouin, D., Beilliard, Y. Unsupervised and efficient learning in sparsely activated convolutional spiking neural networks enabled by voltage-dependent synaptic plasticity. Neuromorph. Comput. Eng. (2023) https://doi.org/10.1088/2634-4386/acad98\nGarg, N., Balafrej, I., Stewart C. T., Portal, JM. Bocquet, M., Querlioz, D., Drouin, D., Rouat, J., Beillard, Y., Alibart, F. Voltage-Dependent Synaptic Plasticity (VDSP): Unsupervised probabilistic Hebbian plasticity rule based on neurons membrane potential. Frontiers in Neuroscience (2022) https://doi.org/10.3389/fnins.2022.983950.\nBalafrej, I., Alibart, F. & Rouat, J. P-CRITICAL: a reservoir autoregulation plasticity rule for neuromorphic hardware. Neuromorph. Comput. Eng. (2022). https://doi.org/10.1088/2634-4386/ac6533.\nGarg, N., Balafrej, I., Beilliard, Y., Drouin, D., Alibart, F., and Rouat, R. Signals to Spikes for Neuromorphic Regulated Reservoir Computing and EMG Hand Gesture Recognition. International Conference on Neuromorphic Systems 2021, 1‚Äì8. ICONS 2021 29. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3477145.3477267.\nCelotti L., Balafrej, I., Calvet, E. Improving Zero-Shot Neural Architecture Search with Parameters Scoring (2020).\n\n\n\nPresentations\n\nGarg, N., Balafrej, I., Beilliard, Y., Drouin, D., Alibart, F., Rouat. Unsupervised Learning with Ferroelectric Synapses. IEEE 52nd European Solid-State Device Research Conference (2022).\nBalafrej, I., Alibart, F. Rouat, J. Neuromorphic Reservoirs for Energy-Efficient Task-Abstract Machine Learning Devices. Neural Interfaces and Systems Symposium (2021).\nBalafrej, I. Building hardware-friendly models for neuromorphic engineering. Workshop at the University of Montreal (2021).\nGarg, N., Balafrej, I., Beilliard, Y., Drouin, D., Alibart, F., and Rouat, R. Signals to Spikes for Neuromorphic Regulated Reservoir Computing and EMG Hand Gesture Recognition. International Conference on Neuromorphic Systems 2021, 1‚Äì8. ICONS 2021 29. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3477145.3477267.\nBalafrej, I., Rouat, J. P-CRITICAL: A Reservoir Autoregulation Plasticity Rule Designed for Loihi. Intel Neuromorphic Research Community Forum (2020).\nBalafrej, I. Machine learning on neuromorphic processors. 88e Congr√®s de l‚ÄôAcfas. (CANCELED - COVID, 2020)\nBalafrej, I. Auto-regulation of Reservoirs with Potential Physiological Signal Monitoring. UNIQUE Symposium (2020)\nBalafrej, I., Rouat, J. Running the CRITICAL Plasticity Rule for Loihi. 3rd Intel Neuromorphic Research Community Workshop, Austria (2019).\nTremblay, J.P., Balafrej, I., Labelle, F., Martel-Denis, F., Matte, √â., Chouinard-Beaupr√©, J., L√©tourneau, A., Mercier-Nicol, A., Brodeur, S., Ferland, F., Rouat, J. A Cooperative Visually Grounded Dialogue Game with a Humanoid Robot. (2018). Demonstration track, Thirty-third Conference on Neural Information Processing Systems (NeurIPS). Abstract"
  },
  {
    "objectID": "posts/2025-02-01.html",
    "href": "posts/2025-02-01.html",
    "title": "The Growing Challenge of Deepfakes and the Future of Detection",
    "section": "",
    "text": "Deepfake technology has advanced rapidly, making it increasingly difficult to distinguish real from manipulated media. Powered by generative AI techniques like GANs and diffusion models, deepfakes can seamlessly alter videos, swap faces, and even clone voices, raising serious concerns about misinformation, identity theft, and political manipulation. As deepfake tools become more accessible, the need for effective detection methods is more urgent than ever.\nMost current deepfake detection methods rely on deep learning models that require significant computational resources, often making them impractical for everyday users. Many solutions depend on cloud-based processing, limiting accessibility and increasing latency. This creates a major gap‚Äîwhile deepfake generation tools are widely available, detection remains out of reach for most people.\nIn this new article, we present a new approach to efficient deepfake detection, addressing the challenge of balancing accuracy with computational efficiency. By leveraging optimized neural networks and innovative inference techniques, we demonstrate how detection can be made more accessible without sacrificing performance.\nDeepfake detection is an ongoing arms race, and scalable, real-time solutions are essential to maintaining trust in digital media. As deepfake generation evolves, so must detection strategies. The future lies in lightweight, adaptive, and multimodal detection systems that can be deployed at scale‚Äîensuring that truth remains distinguishable from fiction.\n\n\n\nCitationBibTeX citation:@online{balafrej2025,\n  author = {Balafrej, Ismael},\n  title = {The {Growing} {Challenge} of {Deepfakes} and the {Future} of\n    {Detection}},\n  date = {2025-02-01},\n  url = {https://ibalafrej.com/posts/2025-02-01.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBalafrej, Ismael. 2025. ‚ÄúThe Growing Challenge of Deepfakes and\nthe Future of Detection.‚Äù February 1, 2025. https://ibalafrej.com/posts/2025-02-01.html."
  },
  {
    "objectID": "posts/2024-04-11-ev-depreciation/index.html",
    "href": "posts/2024-04-11-ev-depreciation/index.html",
    "title": "Are electric cars a worse investment ?",
    "section": "",
    "text": "Electric vehicles (EVs) are generaly more expensive than their gasoline counterpart. Yet, adoption has still be relatively high thanks to governement incentives and the cheap cost of electricity in the province of Quebec. But, as the Roulez Vert program from the government of Quebec is slowly getting phased out, it is worth examining more precicely the financial impact of EVs. The program‚Äôs initial goal was to promote EVs with monetary incentives (up to $7,000 CAD for new cars and $3,500 CAD for used cars). Removing this program means that every EV purchased starting on January 1, 2025, will be more expensive.\nThis presents a unique opportunity for acquiring an EV this year with a lower value depreciation than what would traditionally be expected, as the cost of EVs will slowly increase over the next few years. That is, of course, under the assumption that the cost of new EVs will remain relatively stable over the next few years.\nMy understanding of EVs is that they should hold value longer than your typical gasoline vehicle. There are fewer parts involved in the engine, and the battery capacity can remain relatively stable over a huge number of recharge cycles. The brake pads can be changed less often thanks to the regenerative braking system. Rusting should be roughly similar to gasoline vehicles. The main disadvantage is that battery capacity has been increasing significantly with every new generation of EVs. Therefore, there is an incentive to upgrade your car and thus saturate the used EV market, which decreases prices and depreciates the value of the car faster.\nBefore making any assumptions, it is best to refer to data to see how, with a constant government incentive over the past few years, the EVs in the Montreal market area have been depreciating in value using gasoline vehicles as a baseline."
  },
  {
    "objectID": "posts/2024-04-11-ev-depreciation/index.html#acquiring-used-vehicle-market-data",
    "href": "posts/2024-04-11-ev-depreciation/index.html#acquiring-used-vehicle-market-data",
    "title": "Are electric cars a worse investment ?",
    "section": "Acquiring used vehicle market data",
    "text": "Acquiring used vehicle market data\nTo do so, I first scraped some used vehicle market data using the Web Scraper Chrome extension. I then used the US car models GitHub repository to parse the vehicle make and model out of the text description for each vehicle.\nHere are the first 5 rows of this new dataset:\n\n\nCode\n# Code for parsing us-car-models-data (make and models)\nus_car_models_df = None\nfor file in glob.glob(\"data/us-car-models/*.csv\"):\n    current_year = pd.read_csv(file)\n    if us_car_models_df is None:\n        us_car_models_df = current_year\n    else:\n        us_car_models_df = pd.concat([us_car_models_df, current_year])\n\nall_cars_models = us_car_models_df.model.unique().tolist()\nall_cars_models = sorted(all_cars_models, key=len, reverse=True)\n\n# Code for parsing scaped vehicle data\nevs = pd.read_csv(\"data/electric_cars.csv\")\ngas = pd.read_csv(\"data/gasoline_cars.csv\")\nevs[\"engine_type\"] = \"electric\"\ngas[\"engine_type\"] = \"gasoline\"\ndf = pd.concat([evs, gas]).drop_duplicates()\n\ndf[\"vehicle-name\"] = df[\"vehicle-name\"].str.strip()\ndf[\"year\"] = df[\"vehicle-name\"].str[:4].astype(float)\ndf[\"price\"] = (\n    df[\"price\"].str.extract(\"([\\d,]+)\", expand=False).str.replace(\",\", \"\").astype(float)\n)\n\n\ndf = df[df[\"year\"] &gt; 2010]\ndf[\"make\"] = df[\"vehicle-name\"].str.extract(\"\\d+\\s+([A-Za-z\\-]+)\\s\", expand=False)\n\ndef find_model_in_paragraph(text):\n    reg = re.compile('[^a-z0-9]')\n    for model in all_cars_models:\n        if reg.sub('', model.lower()) in reg.sub('', text.lower()):\n            return model\n    return None\n\ndf[\"model\"] = df[\"vehicle-name\"].apply(find_model_in_paragraph)\n\ndf[\"mileage\"] = df.mileage.str.replace(\"[km,]\", \"\", regex=True).astype(float)\ndf[\"vehicle_age\"] = 2025 - df.year\ndf.drop(\n    [\"vehicle-name\", \"vehicle-desc\", \"pagination\", \"web-scraper-order\", \"web-scraper-start-url\"], axis=1, inplace=True\n) # drop unwanted columns\ndf.dropna(inplace=True)\ndf.year = df.year.astype(int)\n\nprint(df.head(5))\ndf[\"MakeModel\"] = (df[\"make\"] + \" \" + df[\"model\"]).astype(\"category\")\n\n\n&lt;&gt;:23: SyntaxWarning: invalid escape sequence '\\d'\n&lt;&gt;:28: SyntaxWarning: invalid escape sequence '\\d'\n&lt;&gt;:23: SyntaxWarning: invalid escape sequence '\\d'\n&lt;&gt;:28: SyntaxWarning: invalid escape sequence '\\d'\n/tmp/ipykernel_2584/632754913.py:23: SyntaxWarning: invalid escape sequence '\\d'\n  df[\"price\"].str.extract(\"([\\d,]+)\", expand=False).str.replace(\",\", \"\").astype(float)\n/tmp/ipykernel_2584/632754913.py:28: SyntaxWarning: invalid escape sequence '\\d'\n  df[\"make\"] = df[\"vehicle-name\"].str.extract(\"\\d+\\s+([A-Za-z\\-]+)\\s\", expand=False)\n\n\n   mileage    price engine_type  year       make      model  vehicle_age\n0  27707.0  24995.0    electric  2022      Mazda      MX-30          3.0\n1   5923.0  58995.0    electric  2023       Audi  Q4 e-tron          2.0\n2  66068.0  22988.0    electric  2018  Chevrolet    Bolt EV          7.0\n3  45669.0  30985.0    electric  2020        Kia     Optima          5.0\n4  39169.0  38895.0    electric  2022    Hyundai        Ion          3.0"
  },
  {
    "objectID": "posts/2024-04-11-ev-depreciation/index.html#data-visualization",
    "href": "posts/2024-04-11-ev-depreciation/index.html#data-visualization",
    "title": "Are electric cars a worse investment ?",
    "section": "Data visualization",
    "text": "Data visualization\nWe can start our analysis with some visualization of this newly created dataset.\n\n\nCode\nfig, axs = plt.subplots(2, 2, figsize=(10, 8), tight_layout=True)\n\n# Vehicle age\nhist_data = np.unique(df.vehicle_age, return_counts=True)\naxs[0][0].bar(*hist_data, edgecolor=\"black\", align=\"center\")\naxs[0][0].set_xlabel(\"Vehicle age\")\naxs[0][0].set_ylabel(\"Count\")\n\n# Engine type\nhist_data = np.unique(df.engine_type, return_counts=True)\naxs[0][1].bar(*hist_data, edgecolor=\"black\", align=\"center\")\naxs[0][1].set_xlabel(\"Engine type\")\naxs[0][1].set_ylabel(\"Count\")\n\n# Mileage\nbin_size = 5000\nbins, counts = np.unique(df.mileage.astype(int)//bin_size, return_counts=True)\nbins *= bin_size\naxs[1][0].bar(bins/1000, counts, edgecolor=\"black\", align=\"center\", width=bin_size/1000)\naxs[1][0].set_xlabel(\"Mileage [Thousand of km]\")\naxs[1][0].set_ylabel(\"Count\")\n\n# Top 15 auto maker\nbins, counts = np.unique(df.make, return_counts=True)\ntop_idx = np.argsort(counts)[::-1]\nbins = bins[top_idx[:15]]\ncounts = counts[top_idx[:15]]\naxs[1][1].bar(bins, counts, edgecolor=\"black\", align=\"center\")\naxs[1][1].set_xticklabels(bins, rotation=45, ha='right')\naxs[1][1].text(9, 900, \"(Only showing the top 15 auto maker)\", va=\"center\", ha=\"center\")\npass\n\n\n\n\n\nHistogram for each column of the dataset.\n\n\n\n\nTo examine the effect of engine type on vehicle price depreciation, we can calculate both the average and median prices for vehicles of varying ages.\n\n\nCode\nfig, axs = plt.subplots(1, 2, figsize=(10, 5), tight_layout=True, sharey=True)\n\navg_price_by_age = df.groupby(['vehicle_age', 'engine_type']).price.mean().reset_index()\nmed_price_by_age = df.groupby(['vehicle_age', 'engine_type']).price.median().reset_index()\n\n# Plotting\nsns.lineplot(data=avg_price_by_age, x='vehicle_age', y='price', hue='engine_type', marker=\"o\", ax=axs[0])\nsns.lineplot(data=med_price_by_age, x='vehicle_age', y='price', hue='engine_type', marker=\"o\", ax=axs[1])\naxs[0].set_title('Average Vehicle Price by Engine Type')\naxs[1].set_title('Median Vehicle Price by Engine Type')\naxs[0].set_xlabel('Vehicle Age (Years)')\naxs[1].set_xlabel('Vehicle Age (Years)')\naxs[0].set_ylabel('Price ($ CAD)')\naxs[0].legend(title='Engine Type')\npass"
  },
  {
    "objectID": "posts/2024-04-11-ev-depreciation/index.html#modeling",
    "href": "posts/2024-04-11-ev-depreciation/index.html#modeling",
    "title": "Are electric cars a worse investment ?",
    "section": "Modeling",
    "text": "Modeling\nTo validate this conclusion, I created a simple linear mixed-effect regression model to predict vehicle prices using mileage, age, and engine type. There are some biases that are unaccounted for: the options added to the vehicle and the MSRP. We still can get a rough estimate by inspecting the parameters of the learned model of the impact the engine type has on value depreciation.\n\n\nCode\nimport statsmodels.formula.api as smf\n\nmodel = smf.mixedlm(\"price ~ engine_type*mileage + engine_type*vehicle_age\", df, groups=\"MakeModel\").fit()\n\nprint(model.summary())\n\n\n                             Mixed Linear Model Regression Results\n================================================================================================\nModel:                         MixedLM             Dependent Variable:             price        \nNo. Observations:              10521               Method:                         REML         \nNo. Groups:                    619                 Scale:                          89864076.3370\nMin. group size:               1                   Log-Likelihood:                 -113024.6370 \nMax. group size:               239                 Converged:                      Yes          \nMean group size:               17.0                                                             \n------------------------------------------------------------------------------------------------\n                                        Coef.       Std.Err.    z    P&gt;|z|   [0.025     0.975]  \n------------------------------------------------------------------------------------------------\nIntercept                                81613.909  2876.868  28.369 0.000  75975.351  87252.467\nengine_type[T.gasoline]                 -15252.574   902.110 -16.908 0.000 -17020.677 -13484.471\nmileage                                     -0.099     0.007 -13.400 0.000     -0.113     -0.084\nengine_type[T.gasoline]:mileage              0.024     0.008   2.806 0.005      0.007      0.040\nvehicle_age                              -3347.532   158.521 -21.117 0.000  -3658.227  -3036.837\nengine_type[T.gasoline]:vehicle_age        639.121   177.666   3.597 0.000    290.902    987.341\nMakeModel Var                       4744113322.490 29685.324                                    \n================================================================================================\n\n\n\nWhat‚Äôs most interesting about the results are the coefficients. We notice that, as expected and on average, every km driven (mileage) decreases the value of the vehicle by $0.097 and every year added decreases it by $3,333.58. The model uses EVs as a baseline, and we can notice that gasoline vehicles increase the value of the car by $0.022 for every km and $628.86 for every year. Now, these numbers are inexact; the relationship between these variables is most likely not linear. The sign of the coefficient is, however, undeniable: gasoline vehicles hold their value longer than their electrical counterparts."
  },
  {
    "objectID": "posts/2024-04-11-ev-depreciation/index.html#conclusion",
    "href": "posts/2024-04-11-ev-depreciation/index.html#conclusion",
    "title": "Are electric cars a worse investment ?",
    "section": "Conclusion",
    "text": "Conclusion\nIt is evident that EVs undergo greater depreciation than gasoline cars. However, this difference appears primarily in vehicles older than 5 years. Five years ago marked a significant increase in the popularity of EVs in Canada, notably with the release of the Tesla Model 3. The technology has advanced considerably since then, and it is somewhat expected that the early models of EVs did not perform well in the market and are thus resold at lower values."
  },
  {
    "objectID": "posts/2025-05-09.html",
    "href": "posts/2025-05-09.html",
    "title": "Boosting RSNNs for Long-Term Memory",
    "section": "",
    "text": "I‚Äôm excited to share that our latest work, titled ‚ÄúEnhancing temporal learning in recurrent spiking networks for neuromorphic applications‚Äù has just been published in Neuromorphic Computing and Engineering.\nIn this paper, we tackle one of the biggest limitations of training RSNNs with binary spikes, i.e., handling long temporal dependencies. Backpropagation through time becomes increasingly unstable in such settings, but we introduce three key innovations to overcome this:\nSynaptic Delays at the neuron level, helping gradients skip time steps and keeping the firing rate efficient.\nBranching Factor Regularization, inspired by biological systems, which stabilizes dynamics and simplifies training by including a time-local loss component.\nSurrogate Gradient Redesign, where we expand the function‚Äôs support to better accommodate long-range dependencies during learning.\nTogether, these changes dramatically improve performance, as we observed on benchmarks such as the permuted sequential MNIST (psMNIST), where we achieved state-of-the-art results (for spiking models).\nWe believe this work is a step toward making RSNNs more practical and scalable for neuromorphic hardware, both digital and analog. If you‚Äôre working on biologically inspired computing or just curious about the future of efficient neural architectures, I invite you to take a look at the full article.\nhttps://doi.org/10.1088/2634-4386/add293\n\n\n\nCitationBibTeX citation:@online{balafrej2025,\n  author = {Balafrej, Ismael},\n  title = {Boosting {RSNNs} for {Long-Term} {Memory}},\n  date = {2025-09-05},\n  url = {https://ibalafrej.com/posts/2025-05-09.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBalafrej, Ismael. 2025. ‚ÄúBoosting RSNNs for Long-Term\nMemory.‚Äù September 5, 2025. https://ibalafrej.com/posts/2025-05-09.html."
  },
  {
    "objectID": "posts/2023-03-18-dt-vs-lif/index.html",
    "href": "posts/2023-03-18-dt-vs-lif/index.html",
    "title": "Selection of a timestep for SNN simulation",
    "section": "",
    "text": "Context\nI recently came across a tweet by Dan Goodman, which presented a brief experiment demonstrating the detrimental effects of using a large timestep (\\(\\delta t\\)) during the simulation of a LIF neuron. The output spiking rate of a LIF neuron with a Poisson spike input was found to decrease as the timestep increased, with failure observed at soon as \\(\\delta t=1\\) ms - a standard timestep size within the CS-oriented community.\nOf course, there is a direct relationship between the choice of \\(\\delta t\\), and real-world simulation duration (or wall-clock time). Ideally, we would all be using a very large \\(\\delta t\\) for our simulation. As Guillaume Bellec pointed out, there might not even be any advantage in a machine learning setting to using a small \\(\\delta t\\).\nRather than accepting the necessity of a small timestep, it is worth investigating why the simulation fails, even when employing an exact solver instead of Euler‚Äôs method. Specifically, there should only be a small distinction when the spike arrives at the beginning, or the end, of a clock cycle. We somewhat over or underestimate the membrane potential by \\(w\\exp(\\frac{-\\delta t}{\\tau})\\) depending on when the spike arrived during the clock period.\n\n\nRecreating the Simulation\nA straightforward experiment can be devised to replicate the behavior outlined in the tweet. We will simulate 100 LIF neurons, being stimulated by 100 Poisson spike trains sampled at 5 Hz for 4 seconds. The LIF‚Äôs time constant is \\(\\tau=10\\) ms. The weights between the 100 inputs and 100 output neurons are randomly sampled from a normal distribution \\(\\mathcal{N}(0.1, 0.25)\\). We then compute the mean output firing rate of every output neuron, and the corresponding standard deviation as error bars.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Configuration\nnp.random.seed(0x1B)\nduration = 4 # seconds\ntau = 0.010\nthresh = 1\nnb_inputs = 100\nnb_outputs = 1000\ninput_rate = 5 #Hz\nweights = np.random.randn(nb_outputs, nb_inputs)*0.5+0.1\ndts = np.logspace(-5, -1.5, 10) # in seconds\n\n# Simulation\nfig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\nspike_rates = np.zeros((len(dts), nb_outputs)) # output\nfor i, dt in enumerate(dts):\n    time = np.arange(0, duration, dt)\n    u = np.zeros(nb_outputs)\n    _exp = np.exp(-dt/tau)\n    input_spikes = np.random.poisson(lam=input_rate*dt, size=(len(time), nb_inputs))\n    weighted_input_spikes = input_spikes @ weights.T\n    spike_count = 0\n\n    for j, t in enumerate(time):\n        u = _exp * u + weighted_input_spikes[j]\n        spikes = u &gt; thresh\n        spike_count += spikes\n        u[spikes] = 0 # reset\n    spike_rates[i] += spike_count / duration\nax.errorbar(dts*1000, spike_rates.mean(axis=1), yerr=spike_rates.std(axis=1), capsize=5,)\nax.set_xscale(\"log\")\nax.set_xlabel(\"$\\\\delta t$ [ms]\")\nax.set_ylabel(\"Output firing rate [sp/s]\");\n\n\n\n\n\n\n\n\n\nWe arrive at a similar-looking plot, where the output spiking frequency is going down near \\(\\delta t=1\\) ms.\n\n\nHypothesis\nNumerous commenters in the original thread suggested that \\(\\delta t\\) should be chosen in alignment with \\(\\tau\\). Of course, there is some influence of the chosen time constant \\(\\tau\\), as the smaller the leakage during a timestep, the smaller the error of membrane potential that can happen. However, I am skeptical of this notion due to the stochastic nature of Poisson spikes. Given that a spike can occur at any time during a timestep, it seems likely that the overestimation of membrane potential will roughly cancel out the underestimation. My hypothesis differs from this perspective. I contend that the crucial difference lies elsewhere. Specifically, owing to the nature of the simulation, a neuron can only emit a single spike within a given timestep. Consequently, the LIF neuron enters a sort of implicit refractory period for the duration of the timestep. When the timestep is exceedingly large - greater than 1 ms in this instance - the neuron experiences a prolonged refractory period, leading to the potential loss of critical input spikes as it is unable to integrate new input during this interval.\nIf the assumption is correct, i.e the timestep \\(\\delta t\\) if forcing an implicit refractory period, then having a large refractory period but with a smaller \\(\\delta t\\) should yield the same result as having a larger \\(\\delta t\\). If we add a refractory period to the experiment above, we‚Äôll see that they do indeed provide a similar effect:\n\n\nCode\nfig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n\nfor refractory_period in [0.001, 0.01, 0.1]:\n    spike_rates = np.zeros((len(dts), nb_outputs)) # output\n    for i, dt in enumerate(dts):\n        time = np.arange(0, duration, dt)\n        refrac_clk = int(refractory_period/dt)\n        u = np.zeros(nb_outputs)\n        refrac_cntr = np.zeros(nb_outputs, dtype=int)\n        _exp = np.exp(-dt/tau)\n        input_spikes = np.random.poisson(lam=input_rate*dt, size=(len(time), nb_inputs))\n        weighted_input_spikes = input_spikes @ weights.T\n        spike_count = 0\n\n        for j, t in enumerate(time):\n            non_refrac_neurons = refrac_cntr==0\n            u[non_refrac_neurons] = _exp * u[non_refrac_neurons] + weighted_input_spikes[j, non_refrac_neurons]\n            spikes = u &gt; thresh\n            spike_count += spikes\n            u[spikes] = 0 # reset\n\n            # Setup refractory period\n            refrac_cntr = np.maximum(refrac_cntr-1, 0)\n            refrac_cntr[spikes] += refrac_clk\n\n        spike_rates[i] += spike_count / duration\n\n\n    ax.errorbar(dts*1000, spike_rates.mean(axis=1), yerr=spike_rates.std(axis=1), capsize=5, label=f\"Refrac.: {1000*refractory_period:0.1f}ms\")\n    ax.set_xscale(\"log\")\n    ax.set_xlabel(\"$\\\\delta t$ [ms]\")\n    ax.set_ylabel(\"Output firing rate [sp/s]\")\n    ax.legend(loc=\"lower left\")\n\n\n\n\n\n\n\n\n\nAs we see, the output firing rates align when \\(\\delta t\\) is equal to the refractory period. For example, at \\(\\delta t=10\\) ms, the orange line only starts going down when the timestep becomes bigger than the explicit refractory period. Therefore, the model is actually correct. The only difference is that we have to consider that the effective refractory period is equal to the maximum between \\(\\delta t\\) and the explicit refractory period.\n\n\nSolution\nThe solution to this problem is quite simple. As I said before, the timestep of the simulation forces an implicit refractory period because the neuron can only spike once per timstep. If we remove this limitation, then we should remove this implicit refractory period and the output firing rate should be constant regardless of the timestep.\nTo do so, we count the number of times the membrane potential \\(u(t)\\) is above the threshold to estimate how many times the neuron would spike in one timestep. \\(n_{spikes}(t)=\\lfloor \\frac{\\max \\{u(t), 0\\}}{u_{thresh}} \\rfloor\\). We also edit the reset, such that we remove the threshold from the membrane potential \\(n_{spikes}\\) times, referred to as a soft-reset. This reset mechanism is more precise when dealing with large timesteps, as the accumulated membrane potential is not wasted by an early spike during a timestep. We re-simulate the first experiment with this modification, and we obtain:\n\n\nCode\nnp.random.seed(0x1B)\nfig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\nspike_rates = np.zeros((len(dts), nb_outputs)) # output\nfor i, dt in enumerate(dts):\n    time = np.arange(0, duration, dt)\n    u = np.zeros(nb_outputs)\n    _exp = np.exp(-dt/tau)\n    input_spikes = np.random.poisson(lam=input_rate*dt, size=(len(time), nb_inputs))\n    weighted_input_spikes = input_spikes @ weights.T\n    spike_count = 0\n\n    for j, t in enumerate(time):\n        u = _exp * u + weighted_input_spikes[j]\n        #previous code: spikes = u &gt; thresh\n        spikes = np.floor(np.maximum(u, 0) / thresh) # multiple spikes\n        spike_count += spikes\n        u -= spikes*thresh\n        #u[spikes &gt; 0] = 0 \n\n    spike_rates[i] += spike_count / duration\nax.errorbar(dts*1000, spike_rates.mean(axis=1), yerr=spike_rates.std(axis=1), capsize=5,)\nax.set_xscale(\"log\")\nax.set_xlabel(\"$\\\\delta t$ [ms]\")\nax.set_ylabel(\"Output firing rate [sp/s]\");\n\n\n\n\n\n\n\n\n\nAnd voil√†! We get the expected firing rate across all the timesteps. While this solution is very interesting for computational neuroscientists, it partly removes the energy friendliness of spiking neural networks since they are not binary anymore, and the reset involves some arithmetic.\n\n\n\n\nCitationBibTeX citation:@online{balafrej2023,\n  author = {Balafrej, Ismael},\n  title = {Selection of a Timestep for {SNN} Simulation},\n  date = {2023-03-19},\n  url = {https://ibalafrej.com/posts/2023-03-18-dt-vs-lif/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBalafrej, Ismael. 2023. ‚ÄúSelection of a Timestep for SNN\nSimulation.‚Äù March 19, 2023. https://ibalafrej.com/posts/2023-03-18-dt-vs-lif/."
  },
  {
    "objectID": "posts/2023-12-08.html",
    "href": "posts/2023-12-08.html",
    "title": "What if you could grow an AI chip ? üß†",
    "section": "",
    "text": "GPUs are not very efficient for machine learning, but they are dominant in the market because custom chips - with their respective AI models - are expensive to design and manufacture. Imagine if we could grow AI devices using data.\nI‚Äôm thrilled to share that our latest study on the topic has just been published in Nature Communications! üéâ In collaboration with IEMN, 3IT, Universit√© de Sherbrooke and LN2 , we investigate how electropolymerization of PEDOT-PSS fibers can be used to replicate brain-like growth of dendritic connections. This new structural plasticity scheme produces sparse ANNs with no oversampling of connections, making them efficient during training and inference on several machine learning tasks.\nI want to thank all the co-authors - Kamila Janzakova, Ankush Kumar, Nikhil Garg, Corentin Scholaert, Jean Rouat, Dominique Drouin, Yannick Coffinier, S√©bastien Pecqueur and Fabien Alibart - for making this study possible.\nSee the full publication at https://doi.org/10.1038/s41467-023-43887-8.\n\n\n\nCitationBibTeX citation:@online{balafrej2023,\n  author = {Balafrej, Ismael},\n  title = {What If You Could Grow an {AI} Chip\\,? üß†},\n  date = {2023-12-08},\n  url = {https://ibalafrej.com/posts/2023-12-08.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBalafrej, Ismael. 2023. ‚ÄúWhat If You Could Grow an AI Chip‚ÄØ? üß†\n.‚Äù December 8, 2023. https://ibalafrej.com/posts/2023-12-08.html."
  },
  {
    "objectID": "posts/2023-03-20.html",
    "href": "posts/2023-03-20.html",
    "title": "Unexpected coin flip experiment",
    "section": "",
    "text": "Interesting idea from this X post by @littmath:\n\nFlip a fair coin 100 times‚Äîit gives a sequence of heads (H) and tails (T). For each HH in the sequence of flips, Alice gets a point; for each HT, Bob does, so e.g.¬†for the sequence THHHT Alice gets 2 points and Bob gets 1 point. Who is most likely to win?\n\nIf we pause to analyze, it might initially seem that Alice has the upper hand. Consider a perfect scenario: if the sequence were all heads (HHHHHHHHHH), Alice could rack up to 9 points out of 10 flips. On the flip side (pun intended), in a mixed sequence like HTHTHTHTHT, Bob would only manage to score 5 points out of 10. This reasoning suggests Alice might have a better chance of winning, right?\nBut here‚Äôs where it gets intriguing. By conducting a Monte Carlo simulation with N=1M iterations, we uncover a surprising truth: Bob is actually more likely to win:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0x1B)\nN = 1_000_000\nbob_wins = 0\nalice_wins = 0\n\nfor i in range(N):\n    flips = np.random.randint(0, 2, size=100)  # 0: head, 1: tail\n    flips_diff = np.diff(flips)  # 1 for TH, -1 for HT, 0 otherwise\n    points_alice = np.sum((flips_diff == 0) & (flips[:-1] == 0))  # HH\n    points_bob = np.sum(flips_diff == -1)  # HT\n    if points_bob &gt; points_alice:\n        bob_wins += 1\n    elif points_bob &lt; points_alice:\n        alice_wins += 1\n\nfig, ax = plt.subplots(figsize=(4, 3))\nax.bar(\n    [\"Bob wins\", \"Alice wins\"],\n    [bob_wins / N, alice_wins / N],\n    0.5,\n)\nax.set_ylabel(\"Probability\")\nax.set_ylim([0.45, 0.5])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.grid(axis='y')\npass\n\n\n\n\n\nProbability of winning the coin-pair flip game for Bob and Alice.\n\n\n\n\nThe nuance here is that Alice might have the potential to score more points in a single game, but when we look at the bigger picture, Bob wins more frequently. Let‚Äôs enumerate all possible outcomes with just 4 coin flips to understand why:\n\n\nCode\nfrom IPython.display import Markdown\nfrom itertools import product\n\nmd = f\"\"\"\n| Flips | Points Alice | Points Bob | Winner | \n|-------|--------------|------------|--------|\n\"\"\"\n\nfor l in product(\"TH\", repeat=4):\n    flips = \"\".join(l)\n    points_alice = points_bob = 0\n    for pair in zip(flips[:-1], flips[1:]):\n        if pair == (\"H\", \"H\"):\n            points_alice += 1\n        elif pair == (\"H\", \"T\"):\n            points_bob += 1\n    winner = \"Draw\"\n    if points_alice &gt; points_bob:\n        winner = \"Alice\"\n    elif points_alice &lt; points_bob:\n        winner = \"Bob\"\n    md += f\"| {flips} | {points_alice} | {points_bob} | {winner} |\\n\"\n\nMarkdown(md)\n\n\n\n\n\nFlips\nPoints Alice\nPoints Bob\nWinner\n\n\n\n\nTTTT\n0\n0\nDraw\n\n\nTTTH\n0\n0\nDraw\n\n\nTTHT\n0\n1\nBob\n\n\nTTHH\n1\n0\nAlice\n\n\nTHTT\n0\n1\nBob\n\n\nTHTH\n0\n1\nBob\n\n\nTHHT\n1\n1\nDraw\n\n\nTHHH\n2\n0\nAlice\n\n\nHTTT\n0\n1\nBob\n\n\nHTTH\n0\n1\nBob\n\n\nHTHT\n0\n2\nBob\n\n\nHTHH\n1\n1\nDraw\n\n\nHHTT\n1\n1\nDraw\n\n\nHHTH\n1\n1\nDraw\n\n\nHHHT\n2\n1\nAlice\n\n\nHHHH\n3\n0\nAlice\n\n\n\n\n\nThis detailed breakdown showcases Bob‚Äôs advantage, who wins more often than Alice in this simplified scenario. The total number of points in all games for both Alice and Bob is equal, 12. Yet, Bob wins more often by a small margin. This exploration not only demonstrates the surprising outcomes that can emerge from seemingly straightforward situations but also highlights the beauty of Monte Carlo simulations in revealing the unexpected.\n\n\n\nCitationBibTeX citation:@online{balafrej2024,\n  author = {Balafrej, Ismael},\n  title = {Unexpected Coin Flip Experiment},\n  date = {2024-03-20},\n  url = {https://ibalafrej.com/posts/2023-03-20.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBalafrej, Ismael. 2024. ‚ÄúUnexpected Coin Flip Experiment.‚Äù\nMarch 20, 2024. https://ibalafrej.com/posts/2023-03-20.html."
  },
  {
    "objectID": "posts/2023-04-13-defense/index.html",
    "href": "posts/2023-04-13-defense/index.html",
    "title": "PhD thesis defended !",
    "section": "",
    "text": "I‚Äôm thrilled to announce that I have completed my PhD thesis defense! It‚Äôs been a long and challenging journey, but I‚Äôm proud of all the hard work and dedication that I‚Äôve put into this moment.\nAlthough I am not an easily stressed person, I thought it would be interesting to capture the intensity of my experience during the defense. To do so, I wore an electrocardiogram during the entire event. In the plot below, the data is presented with arrows indicating the various moments. During the presentation part, my heart rate remained high but relatively stable. As I answered questions from the committee, my heart rate fluctuated, reflecting the varying levels of stress and anxiety that I experienced.\n\n\n\n\n\nHeart rate in beats per minute during my PhD thesis defense on April 13th, 2023.\n\n\n\n\nThis plot does not reflect any meaningful data and only grossly reflects the physical response of my body, I did find it interesting to see all the bumps directly correlated with the various questions. While these bumps could be stress-induced, they may also only reflect the act of speaking with hand movements increasing the necessary blood flow. I enjoyed gathering the extra data, and now I see this plot as a reminder of my success and my ability to overcome challenges in the future. If you‚Äôre interested in using the data, is it freely available here (HR) and here (ECG).\nI‚Äôm incredibly grateful to my advisors, committee members, friends, and family for their unwavering support throughout this process. Their encouragement and guidance have been invaluable, and I couldn‚Äôt have done this without them. Once again, I‚Äôm proud to have completed my PhD thesis defense and excited to see where this accomplishment takes me in my future professional endeavors."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ismael Balafrej",
    "section": "",
    "text": "Hi, I‚Äôm Ismael. Welcome to my personal Blog. I post content about machine learning, spiking neural networks, data science and more. Feel free to subscribe to the RSS feed."
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "Ismael Balafrej",
    "section": "Latest posts",
    "text": "Latest posts\n\n\n\n\n\n\n\nBoosting RSNNs for Long-Term Memory\n\n1 min\n\npublication\nsnn\n\n\nBecause even spiking neurons deserve a memory boost!\n\n\n\nSep 5, 2025\n\n\n\n\n\n\n\n\n\n\nThe Growing Challenge of Deepfakes and the Future of Detection\n\n2 min\n\npublication\n\n\nDeepfakes are getting smarter; so should detection!\n\n\n\nFeb 1, 2025\n\n\n\n\n\n\n\n\n\n\nAre electric cars a worse investment ?\n\n5 min\n\nData Science\n\n\nThis post examines the value depreciation of electric vehicle (EV) in the Montreal area, with an analysis of the impact of the Quebec government‚Äôs decision to phase out the‚Ä¶\n\n\n\nApr 11, 2024\n\n\n\n\n\n\n\n\n\n\nUnexpected coin flip experiment\n\n3 min\n\nsimulation\nstatistics\n\n\nIn this post, I explore a seemingly straightforward coin flip game between two players. Interestingly, the intuitive approach fails when validated with a Monte Carlo‚Ä¶\n\n\n\nMar 20, 2024\n\n\n\n\n\n\n\n\n\n\nWhat if you could grow an AI chip ? üß†\n\n1 min\n\npublication\n\n\nNew publication alert !\n\n\n\nDec 8, 2023\n\n\n\n\n\n\n\n\n\n\nPhD thesis defended !\n\n4 min\n\nPhD\nECG\n\n\nLike a proper nerd, I wore an ECG during my thesis defense. Here is the data !\n\n\n\nApr 17, 2023\n\n\n\n\n\n\nNo matching items"
  }
]